{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a6b422",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a63ff5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4522af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "641f7a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596c0e9",
   "metadata": {},
   "source": [
    "## Getting a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c24a9",
   "metadata": {},
   "source": [
    "We will use FashionMNIST dataset from torch vision datasets.\n",
    "\n",
    "link to dataset in PyTorch built-in dataset: https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "90aaf69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root= 'data', # where to download data to \n",
    "    train = True,  # do we want the training data? (dataset already formatted)\n",
    "    download = True,\n",
    "    transform = ToTensor(), # how do we want to transform the data\n",
    "    target_transform = None # dont tranform the labels    \n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor(),\n",
    "    target_transform = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f8d43",
   "metadata": {},
   "source": [
    "### Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b776e",
   "metadata": {},
   "source": [
    "#### Check size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "59ab7282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41544b2",
   "metadata": {},
   "source": [
    "#### Common Attributes \n",
    "Exploring the coomon attributes of the train_data (transformed dataset to the format our model accept).\n",
    "\n",
    "`Note:`\n",
    "\n",
    "Built-in datasets from torchvision are already transform. If we will use custom dataset we might need to the `from torchvision import transforms` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af239aaa",
   "metadata": {},
   "source": [
    "#### Class Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5b7d1b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_data.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a5321420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac602301",
   "metadata": {},
   "source": [
    "#### Inspect the first index of our train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e6e1b",
   "metadata": {},
   "source": [
    "`Shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "545024a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_data[0] # first index\n",
    "print(image.shape) # (NCHW format) batchsize, channel, height, width\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a072e",
   "metadata": {},
   "source": [
    "`Note:`\n",
    " - The torch vision accepted format is:\n",
    "1. NCHW (batchsize, channel, height, width)\n",
    "2. NHWC (batchsize, height, width, channel)\n",
    "\n",
    "- In matplotlib the accepted format is in 2 dimension data, we need to squeeze the 'image' to remove the upper dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c0350c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape in 3d (torchvision format): torch.Size([1, 28, 28])\n",
      "The shape required for matplotlib: torch.Size([28, 28]) \n"
     ]
    }
   ],
   "source": [
    "print(f'The shape in 3d (torchvision format): {image.shape}') # 3 dimension\n",
    "print(f'The shape required for matplotlib: {image.squeeze().shape} ') # 2D for visualizing in matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de090cae",
   "metadata": {},
   "source": [
    "`Visualize the first index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a6bbbdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ankle boot')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f'Image shape: {image.shape}')\n",
    "plt.imshow(image.squeeze(), # remove 1 dimension \n",
    "           cmap = 'gray')\n",
    "plt.title(class_names[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a38b14",
   "metadata": {},
   "source": [
    "### Plot more images at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "130f8b66",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(42) # turn on for same results\n",
    "# plotting 16 images\n",
    "# fig = plt.figure(figsize=(9,9))\n",
    "# rows ,cols = 4,4\n",
    "\n",
    "# for i in range(1, rows*cols+1):\n",
    "#     random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
    "#     rand_img, rand_label = train_data[random_idx]\n",
    "#     fig.add_subplot(rows, cols, i)\n",
    "#     plt.imshow(rand_img.squeeze(), cmap='gray')\n",
    "#     plt.title(class_names[rand_label])\n",
    "#     plt.axis(False);             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c706bf9",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "In PyTorch, a data loader is an iterator that provides an efficient way to load and iterate over a dataset during training or evaluation. `It allows you to conveniently divide your dataset into batches`, shuffle the data, and load the data in parallel using multiple workers.\n",
    "\n",
    "Documentation link: https://pytorch.org/docs/stable/data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea63f3",
   "metadata": {},
   "source": [
    "`Common DataLoader Parameters:`\n",
    "\n",
    "- `datasets:` dataset from which to load the data\n",
    "- `batch_size:` how many samples per batch to load (for better hardware performance)\n",
    "- `shuffle:` set to True to have the data reshuffled at every epoch (default: False)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a663ca",
   "metadata": {},
   "source": [
    "`Turn data into batches`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5c7e6ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x272c44d98e0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x272c44d9a30>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Segregate the `train_data` into batches\n",
    "# Set up the HYPERPARAMETER (batch_size)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Train data\n",
    "train_dataloader = DataLoader(\n",
    "    dataset= train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True # remove order\n",
    ")\n",
    "\n",
    "# Test data\n",
    "test_dataloader = DataLoader(\n",
    "    dataset= test_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # don't remove test order\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e84b6dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of train_dataloader: 1875 batches of:32\n",
      "Lenght of test_dataloader: 313 batches of:32\n"
     ]
    }
   ],
   "source": [
    "# print(f'DataLoaders: {train_dataloader, test_dataloader}')\n",
    "print(f'Lenght of train_dataloader: {len(train_dataloader)} batches of:{BATCH_SIZE}')\n",
    "print(f'Lenght of test_dataloader: {len(test_dataloader)} batches of:{BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15762191",
   "metadata": {},
   "source": [
    "- train_dataloader contains 1875 batches of 32 samples/features per batch\n",
    "- test_dataloader contains 131 batches of 32 labels per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15044148",
   "metadata": {},
   "source": [
    "#### Inpect the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c64c4631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the shape of certain batch\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape,  train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49243c9b",
   "metadata": {},
   "source": [
    " - iter(): It enables iterating over the batches of data in the data loader.\n",
    "\n",
    "- next(): The next() function is used to retrieve the next item from the iterator. In this case, it retrieves the next batch of data from the data loader.\n",
    "\n",
    "For example, if you have a data loader that loads images and labels for a classification task, executing next(iter(data_loader)) `will provide you with a batch of images and their corresponding labels.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "06b25047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "print(train_features_batch.shape)\n",
    "print(train_features_batch[31].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc6123",
   "metadata": {},
   "source": [
    "This shows that train_features_batch is a single batch out of 1875 batches. This batch contains 32 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b153473",
   "metadata": {},
   "source": [
    "#### Visualize random sample from a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fcff84ea",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "# retreive one random sample \n",
    "# random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
    "\n",
    "# img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "# plt.imshow(img.squeeze(), cmap='gray')\n",
    "# plt.title(class_names[label])\n",
    "# plt.axis(False)\n",
    "# print(f'Image size: {img.shape}')\n",
    "# print(f'Label: {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e2f5a8",
   "metadata": {},
   "source": [
    "- The size=[1] argument specifies that we want a tensor of size 1, meaning a single random index.\n",
    "\n",
    "- .item(): The .item() method is used to extract the single value from the random index tensor. Since we specified size=[1], the tensor has a single element. Calling .item() on that tensor returns the value as a Python scalar (integer in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56afb5c9",
   "metadata": {},
   "source": [
    "### Flatten()\n",
    "Before we proceed building a model we need to understand that we neeed to reshape our train features batches into a flatten shape.\n",
    "\n",
    "Our model only accept flatten shape.\n",
    "link: https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c582e1c",
   "metadata": {},
   "source": [
    "`Example Demonstration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9f83d496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before flatten: torch.Size([1, 28, 28])\n",
      "shape after flatten: torch.Size([1, 784])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "flatten_model = nn.Flatten()\n",
    "\n",
    "train_features_batch_flat =train_features_batch[0]\n",
    "print(f'shape before flatten: {train_features_batch_flat.shape}')\n",
    "output = flatten_model(train_features_batch_flat) # NHW format -> N H*W\n",
    "print(f'shape after flatten: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1181b9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57375225",
   "metadata": {},
   "source": [
    "# Build a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "84c44ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FashionMNIST_modelV0(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # flatten our data\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bab043",
   "metadata": {},
   "source": [
    "`Create a model instance/object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "107796f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNIST_modelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#setup model with input paramaters\n",
    "model_0 = FashionMNIST_modelV0(input_shape=28*28,\n",
    "                               hidden_units=10,\n",
    "                               output_shape=len(train_data.classes)\n",
    "                              )\n",
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b068a33",
   "metadata": {},
   "source": [
    "### Initial Check\n",
    "This is to verify that our model_0 are working properly.\n",
    "\n",
    "- Create a dummy variable shape in NCHW format(using torch random).\n",
    "- Pass the dummy in our model\n",
    "- Our model will perform the flatten and necesary computation\n",
    "- Our model will result a 10 class showing each classes probability (logits form)\n",
    "- The one with the highest probability is the predicted class of the dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "015e2406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0315,  0.3171,  0.0531, -0.2525,  0.5959,  0.2112,  0.3233,  0.2694,\n",
       "         -0.1004,  0.0157]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_x = torch.rand([1,1,28,28])\n",
    "model_0(dummy_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c8b3c",
   "metadata": {},
   "source": [
    "### Loss Fucntion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "15559f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
    "                           lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c785a5ea",
   "metadata": {},
   "source": [
    "### Accuracy Function\n",
    "\n",
    "We will use our helper function to import accuracy function.Alternatively we can use torch.metrics for accuracy.\n",
    "\n",
    "\n",
    "link: https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c7004ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cf81d1",
   "metadata": {},
   "source": [
    "### Timer (optional)\n",
    "Show how fast our model in training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a51e6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "def print_train_time(start: float,\n",
    "                      end: float,\n",
    "                      device: torch.device=None):\n",
    "    \n",
    "    '''Prints difference between start and end time.'''\n",
    "    total_time = end - start\n",
    "    print(f'Train time on {device}: {total_time:.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310a187",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "- This will train the model in batches \n",
    "- Optimzer will update the models parameter(weight&bias) once per batch instead of per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ddbee8",
   "metadata": {},
   "source": [
    "1. Loop through epochs\n",
    "2. Loop through training batches, perform training steps, calculate the train loss \"per batch\".\n",
    "3. Loop through testing batches, perform test steps, calculate the ttest loss \"per batch\".\n",
    "4. Prints out what's happening\n",
    "5. Time it all (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "21d2080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for progress bar (optional)\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac757e88",
   "metadata": {},
   "source": [
    "### Brace your self this is a homongus code!! take a breather.. inhale.... exhale. ...  inhale... exhale..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "512a9586",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f87490371342beb76dba4cd7f8531e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "------\n",
      "Looked at batch 0/60000 samples\n",
      "Looked at batch 12800/60000 samples\n",
      "Looked at batch 25600/60000 samples\n",
      "Looked at batch 38400/60000 samples\n",
      "Looked at batch 51200/60000 samples\n",
      "\n",
      "Train loss: 0.4041 | Tess loss: 0.4559, Test acc: 84.0555\n",
      "Epoch: 1\n",
      "------\n",
      "Looked at batch 0/60000 samples\n",
      "Looked at batch 12800/60000 samples\n",
      "Looked at batch 25600/60000 samples\n",
      "Looked at batch 38400/60000 samples\n",
      "Looked at batch 51200/60000 samples\n",
      "\n",
      "Train loss: 0.4039 | Tess loss: 0.4571, Test acc: 84.2951\n",
      "Epoch: 2\n",
      "------\n",
      "Looked at batch 0/60000 samples\n",
      "Looked at batch 12800/60000 samples\n",
      "Looked at batch 25600/60000 samples\n",
      "Looked at batch 38400/60000 samples\n",
      "Looked at batch 51200/60000 samples\n",
      "\n",
      "Train loss: 0.4018 | Tess loss: 0.4630, Test acc: 84.1554\n",
      "Train time on cpu: 35.983 seconds\n"
     ]
    }
   ],
   "source": [
    "# Phewww\n",
    "torch.manual_seed(42)\n",
    "train_time_start = timer()\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f'Epoch: {epoch}\\n------')\n",
    "    \n",
    "    ### TRAINING ####\n",
    "    # calculate training loss per batch\n",
    "    train_loss = 0\n",
    "    \n",
    "    # add a loop through training batches (nested loop)\n",
    "    for batch, (X,y) in enumerate(train_dataloader):\n",
    "        # turn into training mode here\n",
    "        model_0.train()\n",
    "        \n",
    "        # forward pass\n",
    "        y_pred = (model_0(X))\n",
    "        \n",
    "        # calculate the loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulate the loss in train_loss\n",
    "        \n",
    "        # optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # loss backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print out what's happening\n",
    "        if batch % 400 == 0:\n",
    "            print(f'Looked at batch {batch * len(X)}/{len(train_dataloader.dataset)} samples')\n",
    "        \n",
    "        \n",
    "    # back to the first loop\n",
    "    train_loss /= len(train_dataloader) \n",
    "    \n",
    "    ### TESTING ####\n",
    "    test_loss, test_acc = 0,0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_test, y_test in test_dataloader:\n",
    "            \n",
    "            # forward pass\n",
    "            test_pred = model_0(X_test)\n",
    "            \n",
    "            # calculate the loss (accumulatively)\n",
    "            test_loss += loss_fn(test_pred, y_test)\n",
    "#             test_loss += test_loss\n",
    "            \n",
    "            # accuracy\n",
    "            test_acc += accuracy_fn(y_test, \n",
    "                                    test_pred.argmax(dim=1)) # argmax our test_pred is in logits, we need to find the highest probability\n",
    "#             tess_acc += tess_acc\n",
    "            \n",
    "            \n",
    "        # calculate the test loss averge per batch\n",
    "        test_loss /= len(test_dataloader)\n",
    "        \n",
    "        # calculate the test acc average per batch\n",
    "        test_acc /= len(test_dataloader)\n",
    "    \n",
    "    # print out what's happening\n",
    "    print(f'\\nTrain loss: {train_loss:.4f} | Tess loss: {test_loss:.4f}, Test acc: {test_acc:.4f}')\n",
    "    \n",
    "# calculate training time\n",
    "train_time_end = timer()        \n",
    "total_train_time_model_0 = print_train_time(start= train_time_start,\n",
    "                                            end= train_time_end,\n",
    "                                            device= str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f060eace",
   "metadata": {},
   "source": [
    "`Training code enterpretztion:`\n",
    "\n",
    "1. for batch, (X,y) in enumerate(train_dataloader)\n",
    "\n",
    " - for batch, (X, y): This line starts the for loop and defines two variables, batch and (X, y). The enumerate() function is used to iterate over the train_dataloader and return both the index (batch number) and the data (X and y) for each iteration.\n",
    "\n",
    " - in train_dataloader: This specifies the object over which the loop will iterate. In this case, it is the train_dataloader object, which is typically used to load batches of training data.\n",
    "\n",
    " - So, during each iteration of the loop, the batch variable will contain the index of the current batch, and (X, y) will contain the actual data from that batch. This structure allows you to access and process the data within the loop, performing training operations or any other desired actions on each batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "587a2171",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43md\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "1d = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9bdde54",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d7bd1d69c441af8e4a3275b9a47b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.44395 | Test loss: 0.46506, Test acc: 83.65%\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.43662 | Test loss: 0.46453, Test acc: 83.89%\n",
      "\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.43045 | Test loss: 0.46594, Test acc: 83.93%\n",
      "\n",
      "Train time on cpu: 38.811 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the seed and start the timer\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "# Set the number of epochs (we'll keep this small for faster training times)\n",
    "epochs = 3\n",
    "\n",
    "# Create training and testing loop\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train() \n",
    "        # 1. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulatively add up the loss per epoch \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print out how many samples have been seen\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    ### Testing\n",
    "    # Setup variables for accumulatively adding up loss and accuracy \n",
    "    test_loss, test_acc = 0, 0 \n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_0(X)\n",
    "           \n",
    "            # 2. Calculate loss (accumatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "# Calculate training time      \n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model_0.parameters()).device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ce70a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5be32e856c047d98cfc41afee3f9294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "------\n",
      "Looked at batch 0/60000 samples\n",
      "Looked at batch 12800/60000 samples\n",
      "Looked at batch 25600/60000 samples\n",
      "Looked at batch 38400/60000 samples\n",
      "Looked at batch 51200/60000 samples\n",
      "\n",
      "Train loss: 0.4265 | Tess loss: 0.4574, Test acc: 84.0056\n",
      "Epoch: 1\n",
      "------\n",
      "Looked at batch 0/60000 samples\n",
      "Looked at batch 12800/60000 samples\n",
      "Looked at batch 25600/60000 samples\n",
      "Looked at batch 38400/60000 samples\n",
      "Looked at batch 51200/60000 samples\n",
      "\n",
      "Train loss: 0.4234 | Tess loss: 0.4596, Test acc: 84.1953\n",
      "Epoch: 2\n",
      "------\n",
      "Looked at batch 0/60000 samples\n",
      "Looked at batch 12800/60000 samples\n",
      "Looked at batch 25600/60000 samples\n",
      "Looked at batch 38400/60000 samples\n",
      "Looked at batch 51200/60000 samples\n",
      "\n",
      "Train loss: 0.4196 | Tess loss: 0.4626, Test acc: 83.9856\n",
      "Train time on cpu: 37.574 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "train_time_start = timer()\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f'Epoch: {epoch}\\n------')\n",
    "    \n",
    "    ### TRAINING ####\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, (X,y) in enumerate(train_dataloader):\n",
    "        model_0.train()\n",
    "        \n",
    "        y_pred = (model_0(X))\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 400 == 0:\n",
    "            print(f'Looked at batch {batch * len(X)}/{len(train_dataloader.dataset)} samples')\n",
    "        \n",
    "        \n",
    "    train_loss /= len(train_dataloader) \n",
    "    \n",
    "    ### TESTING ####\n",
    "    tess_loss, tess_acc = 0,0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_test, y_test in test_dataloader:\n",
    "            \n",
    "            test_pred = model_0(X_test)\n",
    "            \n",
    "            test_loss = loss_fn(test_pred, y_test)  # Highlighted: Initialize test_loss inside the loop\n",
    "            tess_loss += test_loss\n",
    "            \n",
    "            test_acc = accuracy_fn(y_test, test_pred.argmax(dim=1))  # Highlighted: Initialize test_acc inside the loop\n",
    "            tess_acc += test_acc\n",
    "        \n",
    "        tess_loss /= len(test_dataloader)\n",
    "        \n",
    "        tess_acc /= len(test_dataloader)\n",
    "    \n",
    "    print(f'\\nTrain loss: {train_loss:.4f} | Tess loss: {tess_loss:.4f}, Test acc: {tess_acc:.4f}')\n",
    "    \n",
    "# calculate training time\n",
    "train_time_end = timer()        \n",
    "total_train_time_model_0 = print_train_time(start= train_time_start,\n",
    "                                            end= train_time_end,\n",
    "                                            device= str(next(model_0.parameters()).device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c47a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd617d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f0143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7d22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102849bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64498383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa06e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644e39d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0f55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
